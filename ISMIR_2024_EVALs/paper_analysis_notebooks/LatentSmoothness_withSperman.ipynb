{
 "cells": [
  {
   "cell_type": "code",
   "id": "dc3d03bb57e4833a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T07:24:01.202569Z",
     "start_time": "2024-04-11T07:23:59.286822Z"
    }
   },
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%cd ..\n",
    "\n",
    "from ISMIR_2024_EVALs import load_model\n",
    "from model import BaseVAE, MuteVAE, MuteGenreLatentVAE, MuteLatentGenreInputVAE, GenreClassifier\n",
    "from data.src.dataLoaders import Groove2Drum2BarDataset\n",
    "\n",
    "import torch\n"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "down_sampled_ratio=None\n",
    "# load dataset as torch.utils.data.Dataset\n",
    "dataset = Groove2Drum2BarDataset(\n",
    "    dataset_setting_json_path=\"data/dataset_json_settings/Balanced_6000_performed.json\",\n",
    "    subset_tag=\"test\",\n",
    "    max_len=32,\n",
    "    tapped_voice_idx=2,\n",
    "    collapse_tapped_sequence=True,\n",
    "    num_voice_density_bins=3,\n",
    "    num_tempo_bins=6,\n",
    "    num_global_density_bins=7,\n",
    "    augment_dataset=False,\n",
    "    force_regenerate=False\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T07:24:02.498502Z",
     "start_time": "2024-04-11T07:24:01.203336Z"
    }
   },
   "id": "d4200af59b7d9d42",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "model_classifier = load_model(\"./trained_models/genre_classifier.pth\", GenreClassifier)\n",
    "# model_BaseVAE_0_2 = load_model(\"./trained_models/base_vae_beta_0_2.pth\", BaseVAE)\n",
    "# model_BaseVAE_0_5 = load_model(\"./trained_models/base_vae_beta_0_5.pth\", BaseVAE)\n",
    "# model_BaseVAE_1_0 = load_model(\"./trained_models/base_vae_beta_1_0.pth\", BaseVAE)\n",
    "# model_MuteVAE_0_2 = load_model(\"./trained_models/mute_vae_beta_0_2.pth\", MuteVAE)\n",
    "# model_MuteVAE_0_5 = load_model(\"./trained_models/mute_vae_beta_0_5.pth\", MuteVAE)\n",
    "# model_MuteVAE_1_0 = load_model(\"./trained_models/mute_vae_beta_1_0.pth\", MuteVAE)\n",
    "# model_MuteGenreLatentVAE_0_2 = load_model(\"./trained_models/mute_genre_latent_vae_beta_0_2.pth\", MuteGenreLatentVAE)\n",
    "model_MuteGenreLatentVAE_0_5 = load_model(\"./trained_models/mute_genre_latent_vae_beta_0_5.pth\", MuteGenreLatentVAE)\n",
    "# model_MuteGenreLatentVAE_1_0 = load_model(\"./trained_models/mute_genre_latent_vae_beta_1_0.pth\", MuteGenreLatentVAE)\n",
    "# model_MuteLatentGenreInputVAE_0_2 = load_model(\"./trained_models/mute_latent_genre_input_vae_beta_0_2.pth\", MuteLatentGenreInputVAE)\n",
    "# model_MuteLatentGenreInputVAE_0_5 = load_model(\"./trained_models/mute_latent_genre_input_vae_beta_0_5.pth\", MuteLatentGenreInputVAE)\n",
    "# model_MuteLatentGenreInputVAE_1_0 = load_model(\"./trained_models/mute_latent_genre_input_vae_beta_1_0.pth\", MuteLatentGenreInputVAE)\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:24:03.319935Z",
     "start_time": "2024-04-11T07:24:02.499283Z"
    }
   },
   "id": "46ac1762",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\"# Generate Random Styles"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0b9ce50ddcd7b8a"
  },
  {
   "cell_type": "code",
   "source": [
    "latent_dim = 128\n",
    "latent_z_A = torch.randn(1, latent_dim)\n",
    "latent_z_B = torch.randn(1, latent_dim)\n",
    "print(\"Generated latent_z: \")\n",
    "print(\"-\"*50)\n",
    "print(\"re-run this cell to generate a new latent_z\")\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox\n",
    "import IPython.display as ipd\n",
    "from bokeh.embed import file_html\n",
    "from bokeh.resources import CDN\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T07:24:03.324510Z",
     "start_time": "2024-04-11T07:24:03.321439Z"
    }
   },
   "id": "ddbd8b6fc4987a85",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "\n",
    "def interp_between_two_rand_samples(model_, n_interp, sample1_i, sample2_i, genre_ix=None, genre_classifier_model=None):\n",
    "    \n",
    "    sample1 = dataset[sample1_i]\n",
    "    sample2 = dataset[sample2_i]\n",
    "    \n",
    "    if genre_ix is None:\n",
    "        genre_tag = sample1[4].unsqueeze(0)\n",
    "    else:\n",
    "        genre_tag = torch.tensor([genre_ix])\n",
    "    \n",
    "    # kick_is_muted, snare_is_muted, hat_is_muted, tom_is_muted, cymbal_is_muted\n",
    "    kick_is_muted = torch.tensor([0])\n",
    "    snare_is_muted = torch.tensor([0])\n",
    "    hat_is_muted = torch.tensor([0])\n",
    "    tom_is_muted = torch.tensor([0])\n",
    "    cymbal_is_muted = torch.tensor([0])\n",
    "    \n",
    "    if isinstance(model_, BaseVAE):\n",
    "        # get the latent_z for the two samples\n",
    "        _, latent_z1 = model_.predict(\n",
    "            flat_hvo_groove=sample1[0].unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        _, latent_z2 = model_.predict(\n",
    "            flat_hvo_groove=sample2[0].unsqueeze(0)\n",
    "        )\n",
    "    elif isinstance(model_, MuteVAE):\n",
    "        \n",
    "        # get the latent_z for the two samples\n",
    "        _, latent_z1 = model_.predict(\n",
    "            flat_hvo_groove=sample1[0].unsqueeze(0),\n",
    "            kick_is_muted=kick_is_muted,\n",
    "            snare_is_muted=snare_is_muted,\n",
    "            hat_is_muted=hat_is_muted,\n",
    "            tom_is_muted=tom_is_muted,\n",
    "            cymbal_is_muted=cymbal_is_muted\n",
    "        )\n",
    "\n",
    "        _, latent_z2 = model_.predict(\n",
    "            flat_hvo_groove=sample2[0].unsqueeze(0),\n",
    "            kick_is_muted=kick_is_muted,\n",
    "            snare_is_muted=snare_is_muted,\n",
    "            hat_is_muted=hat_is_muted,\n",
    "            tom_is_muted=tom_is_muted,\n",
    "            cymbal_is_muted=cymbal_is_muted\n",
    "        )\n",
    "    elif isinstance(model_, MuteGenreLatentVAE):\n",
    "        _, latent_z1 = model_.predict(\n",
    "            flat_hvo_groove=sample1[0].unsqueeze(0),\n",
    "            genre_tags=genre_tag,\n",
    "            kick_is_muted=kick_is_muted,\n",
    "            snare_is_muted=snare_is_muted,\n",
    "            hat_is_muted=hat_is_muted,\n",
    "            tom_is_muted=tom_is_muted,\n",
    "            cymbal_is_muted=cymbal_is_muted\n",
    "        )\n",
    "        \n",
    "        _, latent_z2 = model_.predict(\n",
    "            flat_hvo_groove=sample2[0].unsqueeze(0),\n",
    "            genre_tags=genre_tag,\n",
    "            kick_is_muted=kick_is_muted,\n",
    "            snare_is_muted=snare_is_muted,\n",
    "            hat_is_muted=hat_is_muted,\n",
    "            tom_is_muted=tom_is_muted,\n",
    "            cymbal_is_muted=cymbal_is_muted\n",
    "        )\n",
    "    elif isinstance(model_, MuteLatentGenreInputVAE):\n",
    "        _, latent_z1 = model_.predict(\n",
    "            flat_hvo_groove=sample1[0].unsqueeze(0),\n",
    "            genre_tags=genre_tag,\n",
    "            kick_is_muted=kick_is_muted,\n",
    "            snare_is_muted=snare_is_muted,\n",
    "            hat_is_muted=hat_is_muted,\n",
    "            tom_is_muted=tom_is_muted,\n",
    "            cymbal_is_muted=cymbal_is_muted\n",
    "        )\n",
    "        \n",
    "        _, latent_z2 = model_.predict(\n",
    "            flat_hvo_groove=sample2[0].unsqueeze(0),\n",
    "            genre_tags=genre_tag,\n",
    "            kick_is_muted=kick_is_muted,\n",
    "            snare_is_muted=snare_is_muted,\n",
    "            hat_is_muted=hat_is_muted,\n",
    "            tom_is_muted=tom_is_muted,\n",
    "            cymbal_is_muted=cymbal_is_muted\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Model not supported\")\n",
    "    \n",
    "    genre_probs, genre_preds = None, None   \n",
    "    \n",
    "    if genre_classifier_model is not None:\n",
    "        genre_probs = []\n",
    "        genre_preds = []\n",
    "        \n",
    "    z_step = (latent_z2 - latent_z1) / (n_interp + 1)\n",
    "    intermediate_latents = []\n",
    "    \n",
    "    # decode\n",
    "    z_s = [latent_z1]\n",
    "    for i in range(1, n_interp + 1):\n",
    "        latent_z = latent_z1 + z_step * i\n",
    "        z_s.append(latent_z)\n",
    "    z_s.append(latent_z2)\n",
    "    \n",
    "    hvo_seqs = []\n",
    "    \n",
    "    for z in z_s:\n",
    "        if isinstance(model_, BaseVAE):\n",
    "            h, v, o = model_.sample(\n",
    "                latent_z = z,\n",
    "                voice_thresholds=torch.tensor([0.5] * 9),\n",
    "                voice_max_count_allowed=torch.tensor([32] * 9),\n",
    "            )\n",
    "        elif isinstance(model_, MuteVAE):\n",
    "            h, v, o = model_.sample(\n",
    "                latent_z = z,\n",
    "                kick_is_muted=kick_is_muted,\n",
    "                snare_is_muted=snare_is_muted,\n",
    "                hat_is_muted=hat_is_muted,\n",
    "                tom_is_muted=tom_is_muted,\n",
    "                cymbal_is_muted=cymbal_is_muted,\n",
    "                voice_thresholds=torch.tensor([0.5] * 9),\n",
    "                voice_max_count_allowed=torch.tensor([32] * 9),\n",
    "                sampling_mode=0\n",
    "            )\n",
    "        elif isinstance(model_, MuteGenreLatentVAE):\n",
    "            h, v, o = model_.sample(\n",
    "                latent_z = z,\n",
    "                genre=genre_tag,\n",
    "                kick_is_muted=kick_is_muted,\n",
    "                snare_is_muted=snare_is_muted,\n",
    "                hat_is_muted=hat_is_muted,\n",
    "                tom_is_muted=tom_is_muted,\n",
    "                cymbal_is_muted=cymbal_is_muted,\n",
    "                voice_thresholds=torch.tensor([0.5] * 9),\n",
    "                voice_max_count_allowed=torch.tensor([32] * 9),\n",
    "                sampling_mode=0\n",
    "            )\n",
    "        elif isinstance(model_, MuteLatentGenreInputVAE):\n",
    "            h, v, o = model_.sample(\n",
    "                latent_z = z,\n",
    "                kick_is_muted=kick_is_muted,\n",
    "                snare_is_muted=snare_is_muted,\n",
    "                hat_is_muted=hat_is_muted,\n",
    "                tom_is_muted=tom_is_muted,\n",
    "                cymbal_is_muted=cymbal_is_muted,\n",
    "                voice_thresholds=torch.tensor([0.5] * 9),\n",
    "                voice_max_count_allowed=torch.tensor([32] * 9),\n",
    "                sampling_mode=0\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Model not supported\")\n",
    "            \n",
    "        hvo = torch.cat([h, v, o], dim=2)\n",
    "        \n",
    "        hvo_seq = dataset.hvo_sequences[sample1_i].copy_empty()\n",
    "        hvo_seq.hvo = hvo[0, :, :].squeeze().detach().cpu().numpy()\n",
    "        hvo_seqs.append(hvo_seq)\n",
    "        \n",
    "        if genre_classifier_model is not None:\n",
    "            genre_classifier_model.eval()\n",
    "            with torch.no_grad():\n",
    "                ix, prob = genre_classifier_model.predict(hvo)\n",
    "                genre_probs.append(prob)\n",
    "                genre_preds.append(ix)\n",
    "                \n",
    "    # append HVOS\n",
    "    hvo_seq_all = None\n",
    "    \n",
    "    for ix, h_s in enumerate(hvo_seqs):\n",
    "        if ix == 0:\n",
    "            hvo_seq_all = h_s\n",
    "        else:\n",
    "            hvo_seq_all = hvo_seq_all + h_s\n",
    "    \n",
    "    if genre_classifier_model is not None:\n",
    "        genre_preds = [p.item() for p in genre_preds]\n",
    "\n",
    "        probs = []\n",
    "        for g_p in genre_probs:\n",
    "            probs.append([p.numpy().tolist() for p in g_p] )\n",
    "        \n",
    "        \n",
    "    return hvo_seq_all, hvo_seqs, probs, genre_preds\n",
    "\n",
    "\n",
    "def interp_between_two_rand_samples_with_controls(model_, n_interp, sample1_i, sample2_i, genre_classifier_model=None, mutes1=None, mutes2=None):\n",
    "    \n",
    "    sample1 = dataset[sample1_i]\n",
    "    sample2 = dataset[sample2_i]\n",
    "    \n",
    "    genre1 = sample1[4].unsqueeze(0)\n",
    "    genre2 = sample2[4].unsqueeze(0)\n",
    "    \n",
    "    mutes1 = torch.tensor([sample1[8], sample1[9], sample1[10], sample1[11], sample1[12]]) if mutes1 is None else mutes1\n",
    "    mutes2 = torch.tensor([sample2[8], sample2[9], sample2[10], sample2[11], sample2[12]]) if mutes2 is None else mutes2\n",
    "    \n",
    "    assert isinstance(model_, MuteGenreLatentVAE) or isinstance(model_, MuteVAE), \"Model must be an instance of MuteGenreLatentVAE or MuteVAE\"\n",
    "    if isinstance(model_, MuteVAE):\n",
    "        \n",
    "        # get the latent_z for the two samples\n",
    "        _, latent_z1 = model_.predict(\n",
    "            flat_hvo_groove=sample1[0].unsqueeze(0),\n",
    "            kick_is_muted=mutes1[0],\n",
    "            snare_is_muted=mutes1[1],\n",
    "            hat_is_muted=mutes1[2],\n",
    "            tom_is_muted=mutes1[3],\n",
    "            cymbal_is_muted=mutes1[4]\n",
    "        )\n",
    "\n",
    "        _, latent_z2 = model_.predict(\n",
    "            flat_hvo_groove=sample2[0].unsqueeze(0),\n",
    "            kick_is_muted=mutes2[0],\n",
    "            snare_is_muted=mutes2[1],\n",
    "            hat_is_muted=mutes2[2],\n",
    "            tom_is_muted=mutes2[3],\n",
    "            cymbal_is_muted=mutes2[4]\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Model must be an instance of BaseVAE or MuteVAE\")\n",
    "    \n",
    "    genre_probs, genre_preds = None, None   \n",
    "    \n",
    "    if genre_classifier_model is not None:\n",
    "        genre_probs = []\n",
    "        genre_preds = []\n",
    "        \n",
    "    z_step = (latent_z2 - latent_z1) / (n_interp + 1)\n",
    "    intermediate_latents = []\n",
    "    \n",
    "    # decode\n",
    "    hvo_seqs = []\n",
    "    \n",
    "    for i in range(0, n_interp + 2):\n",
    "        if isinstance(model_, MuteVAE):\n",
    "            h, v, o = model_.interpolate_2d(\n",
    "                interpolation_factor=i / (n_interp + 1),\n",
    "                latent_z_1=latent_z1,\n",
    "                mute_controls_1=mutes1,\n",
    "                latent_z_2=latent_z2,\n",
    "                mute_controls_2=mutes2,\n",
    "                voice_thresholds=torch.tensor([0.5] * 9),\n",
    "                voice_max_count_allowed=torch.tensor([32] * 9),\n",
    "                sampling_mode=0\n",
    "            )\n",
    "        \n",
    "            \n",
    "        hvo = torch.cat([h, v, o], dim=2)\n",
    "        \n",
    "        hvo_seq = dataset.hvo_sequences[sample1_i].copy_empty()\n",
    "        hvo_seq.hvo = hvo[0, :, :].squeeze().detach().cpu().numpy()\n",
    "        hvo_seqs.append(hvo_seq)\n",
    "        \n",
    "        if genre_classifier_model is not None:\n",
    "            genre_classifier_model.eval()\n",
    "            with torch.no_grad():\n",
    "                ix, prob = genre_classifier_model.predict(hvo)\n",
    "                genre_probs.append(prob)\n",
    "                genre_preds.append(ix)\n",
    "                \n",
    "    # append HVOS\n",
    "    hvo_seq_all = None\n",
    "    \n",
    "    for ix, h_s in enumerate(hvo_seqs):\n",
    "        if ix == 0:\n",
    "            hvo_seq_all = h_s\n",
    "        else:\n",
    "            hvo_seq_all = hvo_seq_all + h_s\n",
    "    \n",
    "    if genre_classifier_model is not None:\n",
    "        genre_preds = [p.item() for p in genre_preds]\n",
    "\n",
    "        probs = []\n",
    "        for g_p in genre_probs:\n",
    "            probs.append([p.numpy().tolist() for p in g_p] )\n",
    "        \n",
    "        \n",
    "    return hvo_seq_all, hvo_seqs, probs, genre_preds\n",
    "\n",
    "\n",
    "\n",
    "def calculate_area_under_piecewise_lines(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the area under the piecewise linear function defined by x and y\n",
    "    \"\"\"\n",
    "    area = 0\n",
    "    for i in range(1, len(x)):\n",
    "        area += (x[i] - x[i-1]) * (y[i] + y[i-1]) / 2\n",
    "    return area\n",
    "\n",
    "# jaccard similarity\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "\n",
    "def jaccard_distance(h1, h2):\n",
    "\n",
    "    val = jaccard_score(h1.flatten(), h2.flatten(), average=\"binary\")\n",
    "    # 2 floating point precision\n",
    "    if np.isnan(val):\n",
    "        return 1\n",
    "    else:\n",
    "        return round(1 - val, 2)\n",
    "    \n",
    "\n",
    "def calculate_relative_jackards(hvo_seqs_, rhythm_flat=False):\n",
    "    if rhythm_flat:\n",
    "        hvo_seqs = [hvo_seq.copy() for hvo_seq in hvo_seqs_]\n",
    "        for hvo_seq in hvo_seqs:\n",
    "            hvo_seq.hvo = hvo_seq.flatten_voices()\n",
    "    else:\n",
    "        hvo_seqs = hvo_seqs_\n",
    "        \n",
    "    AB_Jaccard = jaccard_distance(hvo_seqs[0].hits, hvo_seqs[-1].hits)\n",
    "    \n",
    "    if AB_Jaccard == 0:\n",
    "        return None, None, None\n",
    "    \n",
    "    relative_jaccards = []\n",
    "    interp_labels = []\n",
    "    is_ = [\"A\", r\"$I_1$\", r\"$I_2$\", r\"$I_3$\", r\"$I_4$\", r\"$I_5$\", r\"$I_6$\", r\"$I_7$\", r\"$I_8$\", r\"$I_9$\", r\"$I_{10}$\", \"B\"]\n",
    "\n",
    "    for ix, hvo_s in enumerate(hvo_seqs):\n",
    "        if len(hvo_s.hits) == len(hvo_seqs[0].hits):\n",
    "            A_Here_Jaccard = jaccard_distance(hvo_s.hits, hvo_seqs[0].hits) / AB_Jaccard\n",
    "        \n",
    "        # normalize\n",
    "        relative_jaccards.append(np.round(A_Here_Jaccard , 2))\n",
    "        interp_labels.append(is_[ix])\n",
    "    \n",
    "    interp_labels[0] = \"A\"\n",
    "    interp_labels[-1] = \"B\"\n",
    "    \n",
    "    interp_factors = np.linspace(0, 1, len(hvo_seqs))\n",
    "    \n",
    "    return interp_factors, relative_jaccards, interp_labels\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_sim_hvo(h1, h2):\n",
    "    h1 = torch.tensor(h1)\n",
    "    h2 = torch.tensor(h2)\n",
    "    return cosine_similarity(h1.flatten().unsqueeze(0), h2.flatten().unsqueeze(0))\n",
    "\n",
    "def calculate_relative_cosine_distance(hvo_seqs):\n",
    "    AB_Cosine = 1 - cosine_sim_hvo(hvo_seqs[0].get(\"v\"), hvo_seqs[-1].get(\"v\"))\n",
    "    \n",
    "    relative_cosines = []\n",
    "    interp_labels = []\n",
    "    is_ = [\"A\", r\"$I_1$\", r\"$I_2$\", r\"$I_3$\", r\"$I_4$\", r\"$I_5$\", r\"$I_6$\", r\"$I_7$\", r\"$I_8$\", r\"$I_9$\", r\"$I_{10}$\", \"B\"]\n",
    "    \n",
    "    for ix, hvo_s in enumerate(hvo_seqs):\n",
    "        A_Here_Cosine = (1 - cosine_sim_hvo(hvo_s.get(\"v\"), hvo_seqs[0].get(\"v\"))) / AB_Cosine\n",
    "        # normalize\n",
    "        relative_cosines.append(np.round(A_Here_Cosine.item() , 2))\n",
    "        interp_labels.append(is_[ix])\n",
    "    \n",
    "    interp_labels[0] = \"A\"\n",
    "    interp_labels[-1] = \"B\"\n",
    "    \n",
    "    interp_factors = np.linspace(0, 1, len(hvo_seqs))\n",
    "    \n",
    "    return interp_factors, relative_cosines, interp_labels\n",
    "\n",
    "def calculate_vel_MSE_distance(hvo_seqs):\n",
    "    AB_MSE = torch.nn.MSELoss(reduction='none')(torch.tensor(hvo_seqs[0].get(\"v\")), torch.tensor(hvo_seqs[-1].get(\"v\"))).sum()\n",
    "    \n",
    "    relative_MSEs = []\n",
    "    interp_labels = []\n",
    "    is_ = [\"A\", r\"$I_1$\", r\"$I_2$\", r\"$I_3$\", r\"$I_4$\", r\"$I_5$\", r\"$I_6$\", r\"$I_7$\", r\"$I_8$\", r\"$I_9$\", r\"$I_{10}$\", \"B\"]\n",
    "    \n",
    "    for ix, hvo_s in enumerate(hvo_seqs):\n",
    "        A_Here_MSE = torch.nn.MSELoss(reduction='none')(torch.tensor(hvo_s.get(\"v\")), torch.tensor(hvo_seqs[0].get(\"v\"))).sum() / AB_MSE\n",
    "        # normalize\n",
    "        relative_MSEs.append(np.round(A_Here_MSE.item() , 2))\n",
    "        interp_labels.append(is_[ix])\n",
    "    \n",
    "    interp_labels[0] = \"A\"\n",
    "    interp_labels[-1] = \"B\"\n",
    "    \n",
    "    \n",
    "    interp_factors = np.linspace(0, 1, len(hvo_seqs))\n",
    "    \n",
    "    return interp_factors, relative_MSEs, interp_labels\n",
    "\n",
    "def plot_distance(interp_factors, relative_distances, interp_labels):\n",
    "    \n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.plot(interp_factors, relative_distances, 'o-')\n",
    "    # draw a dashed line (45 degree line)\n",
    "    plt.plot([interp_factors[0], interp_factors[-1]], [0, 1], 'k--')\n",
    "    \n",
    "    # fill between the lines\n",
    "    plt.fill_between(interp_factors, interp_factors, relative_distances, alpha=0.1)\n",
    "    \n",
    "    # use the labels on x-axis\n",
    "    plt.xticks(interp_factors, interp_labels)\n",
    "    plt.yticks([*interp_factors, 1], np.round([*interp_factors, 1], 2))\n",
    "    \n",
    "    # rotate the xticks by 45 degrees\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "def calculate_deviation_area(interp_factors, vals, normalize=False):\n",
    "    \"\"\"\n",
    "    Calculate the difference between the area under the 45 degree line (i.e. perfect interpolation) and the area under the actual interpolation\n",
    "    \"\"\"\n",
    "    ideal_area = calculate_area_under_piecewise_lines([interp_factors[0], interp_factors[-1]], [0, 1])\n",
    "    actual_area = calculate_area_under_piecewise_lines(interp_factors, vals)\n",
    "    \n",
    "    if normalize:\n",
    "        return (ideal_area - actual_area) / ideal_area\n",
    "    else:\n",
    "        return ideal_area - actual_area\n",
    "\n",
    "def calculate_total_error(vals, normalize=True):\n",
    "    \"\"\"\n",
    "       vals should be close to interp_factors\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "       vals should be close to interp_factors\n",
    "    \"\"\"\n",
    "    # worst case scenario --> all values are 1\n",
    "    target = np.linspace(0, 1, len(vals))\n",
    "    most_error = np.sum(np.abs(target - 1))\n",
    "    if normalize:\n",
    "        return np.sum(np.abs(target - vals)) / most_error\n",
    "    else:\n",
    "        return np.sum(np.abs(target - vals))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T07:24:03.368048Z",
     "start_time": "2024-04-11T07:24:03.325261Z"
    }
   },
   "id": "f966eab7541c49f3",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import IPython.display\n",
    "# heatmap of the genre probabilities\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "n_interp = 10\n",
    "\n",
    "hvo_seq_all, hvo_seqs, genre_probs, genre_preds = interp_between_two_rand_samples(model_MuteGenreLatentVAE_0_5, n_interp, int(np.random.randint(0, len(dataset))), int(np.random.randint(0, len(dataset))), genre_ix=None, genre_classifier_model=model_classifier)\n",
    "\n",
    "# mutes1 = torch.tensor([1, 1, 0, 0, 0])\n",
    "# mutes2 = torch.tensor([0, 0, 0, 0, 0])\n",
    "# hvo_seq_all, hvo_seqs, genre_probs, genre_preds = interp_between_two_rand_samples_with_controls(model_MuteVAE_0_5, n_interp, int(np.random.randint(0, len(dataset))), int(np.random.randint(0, len(dataset))), genre_classifier_model=model_classifier, mutes1=None, mutes2=None)\n",
    "\n",
    "interp_factors, relative_jaccards, interp_labels = calculate_relative_jackards(hvo_seqs)\n",
    "interp_factors_cosine, relative_cosines, interp_labels_cosine = calculate_relative_cosine_distance(hvo_seqs)\n",
    "interp_factors_MSE, relative_MSEs, interp_labels_MSE = calculate_vel_MSE_distance(hvo_seqs)\n",
    "\n",
    "print(calculate_deviation_area(relative_jaccards, interp_factors, normalize=True), calculate_deviation_area(relative_cosines, interp_factors_cosine, normalize=True), calculate_deviation_area(relative_MSEs, interp_factors_MSE, normalize=True))\n",
    "print(calculate_total_error(relative_jaccards), calculate_total_error(relative_cosines), calculate_total_error(relative_MSEs))\n",
    "\n",
    "\n",
    "\n",
    "genre_probs = np.array(genre_probs)\n",
    "genre_probs = genre_probs.squeeze()\n",
    "\n",
    "genre_probs_df = pd.DataFrame(genre_probs)\n",
    "\n",
    "# axis labels\n",
    "genre_probs_df.columns = dataset.genre_tags\n",
    "\n",
    "# interp labels\n",
    "genre_probs_df.index = interp_labels\n",
    "\n",
    "\n",
    "# transpose\n",
    "genre_probs_df = genre_probs_df.T\n",
    "sns.heatmap(genre_probs_df, annot=True, fmt=\".1f\", cmap=\"YlGnBu\")\n",
    "\n",
    "# plot\n",
    "pr = hvo_seq_all.piano_roll(width=1400, height=300)\n",
    "# convert bokeh figure to html\n",
    "html = file_html(pr, CDN, \"my plot\")\n",
    "audio = hvo_seq_all.synthesize(sf_path=\"hvo_sequence/soundfonts/Standard_Drum_Kit.sf2\")\n",
    "IPython.display.display(ipd.Audio(audio, rate=44100))\n",
    "# show html\n",
    "IPython.display.display(IPython.display.HTML(html))\n",
    "# plot_distance(interp_factors, relative_jaccards, interp_labels)\n",
    "# plot_distance(interp_factors_cosine, relative_cosines, interp_labels_cosine)\n",
    "plot_distance(interp_factors_MSE, relative_MSEs, interp_labels_MSE)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T07:24:06.718462Z",
     "start_time": "2024-04-11T07:24:03.369670Z"
    }
   },
   "id": "d9b86b29c589236b",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:24:06.721382Z",
     "start_time": "2024-04-11T07:24:06.719062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get 100 indices for each genre (0-8)\n",
    "genres = dataset.genre_targets.numpy()\n",
    "\n",
    "genre_indices = []\n",
    "for genre in range(9):\n",
    "    genre_indices.append(np.where(genres == genre)[0])\n",
    "    \n",
    "genre_indices = np.array(genre_indices)\n"
   ],
   "id": "9b21e77f8bb4dea6",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:24:13.065491Z",
     "start_time": "2024-04-11T07:24:06.721937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import product\n",
    "\n",
    "genre_index_pairs = {i : [] for i in range(9)}\n",
    "\n",
    "# sample non-repeating pairs per genre\n",
    "for genre in range(9):\n",
    "    # remove equal pairs\n",
    "    pairs = [p for p in list(product(genre_indices[genre], genre_indices[genre])) if p[0] != p[1]]\n",
    "    # shuffle and get the first 100\n",
    "    np.random.shuffle(pairs)\n",
    "    genre_index_pairs[genre] = pairs[:300]\n",
    "\n",
    "len(genre_index_pairs)"
   ],
   "id": "ec708e8faa693e29",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:24:13.319326Z",
     "start_time": "2024-04-11T07:24:13.066195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "genre_ix = 0\n",
    "pair_ix = 95\n",
    "\n",
    "def flatten_hvoseqs(hvo_seqs):\n",
    "    hvo_seqs_flat = []\n",
    "    for hvo_seq in hvo_seqs:\n",
    "        hvo_seq_flat = hvo_seq.copy()\n",
    "        hvo_seq_flat.hvo = hvo_seq.flatten_voices()\n",
    "        hvo_seqs_flat.append(hvo_seq_flat)\n",
    "    return hvo_seqs_flat\n",
    "\n",
    "hvo_seq_all, hvo_seqs, genre_probs, genre_preds = interp_between_two_rand_samples(model_MuteGenreLatentVAE_0_5, n_interp, int(genre_index_pairs[genre_ix][pair_ix][0]), int(genre_index_pairs[genre_ix][pair_ix][1]), genre_ix=genre_ix, genre_classifier_model=model_classifier)\n",
    "flat_hvo_seqs = flatten_hvoseqs(hvo_seqs)\n",
    "\n",
    "interp_factors, relative_jaccards, interp_labels = calculate_relative_jackards(flat_hvo_seqs)\n",
    "interp_factors_cosine, relative_cosines, interp_labels_cosine = calculate_relative_cosine_distance(hvo_seqs)\n",
    "interp_factors_MSE, relative_MSEs, interp_labels_MSE = calculate_vel_MSE_distance(hvo_seqs)\n",
    "\n"
   ],
   "id": "75d0e742c070c6e7",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:24:13.325961Z",
     "start_time": "2024-04-11T07:24:13.320515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_all_distances(model_, n_interp, genre_index_pairs, rhythm_flat=False):\n",
    "    step_I1_jaccard_cosinevel_msevels = []\n",
    "    step_I2_jaccard_cosinevel_msevels = []\n",
    "    step_I3_jaccard_cosinevel_msevels = []\n",
    "    step_I4_jaccard_cosinevel_msevels = []\n",
    "    step_I5_jaccard_cosinevel_msevels = []\n",
    "    step_I6_jaccard_cosinevel_msevels = []\n",
    "    step_I7_jaccard_cosinevel_msevels = []\n",
    "    step_I8_jaccard_cosinevel_msevels = []\n",
    "    step_I9_jaccard_cosinevel_msevels = []\n",
    "    step_I10_jaccard_cosinevel_msevels = []\n",
    "    \n",
    "    for genre_ix in tqdm(range(9)):\n",
    "        for pair_ix in range(200):\n",
    "            hvo_seq_all, hvo_seqs, genre_probs, genre_preds = interp_between_two_rand_samples(model_, n_interp, int(genre_index_pairs[genre_ix][pair_ix][0]), int(genre_index_pairs[genre_ix][pair_ix][1]), genre_ix=genre_ix, genre_classifier_model=model_classifier)\n",
    "            flat_hvo_seqs = flatten_hvoseqs(hvo_seqs)\n",
    "            interp_factors, relative_jaccards, interp_labels = calculate_relative_jackards(flat_hvo_seqs, rhythm_flat=rhythm_flat)\n",
    "            if relative_jaccards is not None:\n",
    "                interp_factors_MSE, relative_MSEs, interp_labels_MSE = calculate_vel_MSE_distance(hvo_seqs)\n",
    "                step_I1_jaccard_cosinevel_msevels.append([relative_jaccards[1], None, relative_MSEs[1]])\n",
    "                step_I2_jaccard_cosinevel_msevels.append([relative_jaccards[2], None, relative_MSEs[2]])\n",
    "                step_I3_jaccard_cosinevel_msevels.append([relative_jaccards[3], None, relative_MSEs[3]])\n",
    "                step_I4_jaccard_cosinevel_msevels.append([relative_jaccards[4], None, relative_MSEs[4]])\n",
    "                step_I5_jaccard_cosinevel_msevels.append([relative_jaccards[5], None, relative_MSEs[5]])\n",
    "                step_I6_jaccard_cosinevel_msevels.append([relative_jaccards[6], None, relative_MSEs[6]])\n",
    "                step_I7_jaccard_cosinevel_msevels.append([relative_jaccards[7], None, relative_MSEs[7]])\n",
    "                step_I8_jaccard_cosinevel_msevels.append([relative_jaccards[8], None, relative_MSEs[8]])\n",
    "                step_I9_jaccard_cosinevel_msevels.append([relative_jaccards[9], None, relative_MSEs[9]])\n",
    "                step_I10_jaccard_cosinevel_msevels.append([relative_jaccards[10], None, relative_MSEs[10]])\n",
    "            \n",
    "    step_1_all = np.array(step_I1_jaccard_cosinevel_msevels)\n",
    "    step_2_all = np.array(step_I2_jaccard_cosinevel_msevels)\n",
    "    step_3_all = np.array(step_I3_jaccard_cosinevel_msevels)\n",
    "    step_4_all = np.array(step_I4_jaccard_cosinevel_msevels)\n",
    "    step_5_all = np.array(step_I5_jaccard_cosinevel_msevels)\n",
    "    step_6_all = np.array(step_I6_jaccard_cosinevel_msevels)\n",
    "    step_7_all = np.array(step_I7_jaccard_cosinevel_msevels)\n",
    "    step_8_all = np.array(step_I8_jaccard_cosinevel_msevels)\n",
    "    step_9_all = np.array(step_I9_jaccard_cosinevel_msevels)\n",
    "    step_10_all = np.array(step_I10_jaccard_cosinevel_msevels)\n",
    "    \n",
    "    return step_1_all, step_2_all, step_3_all, step_4_all, step_5_all, step_6_all, step_7_all, step_8_all, step_9_all, step_10_all\n"
   ],
   "id": "ce845db7eef54718",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:29:37.645992Z",
     "start_time": "2024-04-11T07:29:37.636873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def interpolation_plot(model_name, beta, axs):\n",
    "    def get_model(model_name, beta):\n",
    "        # if model_name == \"Base\":\n",
    "        #     if beta == 0.2:\n",
    "        #         return model_BaseVAE_0_2\n",
    "        #     elif beta == 0.5:\n",
    "        #         return model_BaseVAE_0_5\n",
    "        #     elif beta == 1.0:\n",
    "        #         return model_BaseVAE_1_0\n",
    "        # elif model_name == \"Mute\":\n",
    "        #     if beta == 0.2:\n",
    "        #         return model_MuteVAE_0_2\n",
    "        #     elif beta == 0.5:\n",
    "        #         return model_MuteVAE_0_5\n",
    "        #     elif beta == 1.0:\n",
    "        #         return model_MuteVAE_1_0\n",
    "        # elif model_name == \"MuteGenre1\":\n",
    "        #     if beta == 0.2:\n",
    "        #         return model_MuteGenreLatentVAE_0_2\n",
    "        #     elif beta == 0.5:\n",
    "        #         return model_MuteGenreLatentVAE_0_5\n",
    "        #     elif beta == 1.0:\n",
    "        #         return model_MuteGenreLatentVAE_1_0\n",
    "        # elif model_name == \"MuteGenre2\":\n",
    "        #     if beta == 0.2:\n",
    "        #         return model_MuteLatentGenreInputVAE_0_2\n",
    "        #     elif beta == 0.5:\n",
    "        #         return model_MuteLatentGenreInputVAE_0_5\n",
    "        #     elif beta == 1.0:\n",
    "        #         return model_MuteLatentGenreInputVAE_1_0\n",
    "        # else:\n",
    "        #     raise ValueError(\"Invalid model name\")\n",
    "        return model_MuteGenreLatentVAE_0_5\n",
    "    \n",
    "    step_1_all, step_2_all, step_3_all, step_4_all, step_5_all, step_6_all, step_7_all, step_8_all, step_9_all, step_10_all = calculate_all_distances(get_model(model_name, beta), n_interp, genre_index_pairs, rhythm_flat=True)\n",
    "    \n",
    "    \n",
    "    for feature_ix in [0, 2]:\n",
    "        if feature_ix == 0:\n",
    "            ax = axs[0]\n",
    "        else:\n",
    "            ax = axs[1]\n",
    "            \n",
    "        # boxplot for each step\n",
    "        boxprops = dict(linestyle='--', linewidth=0.5, color='darkgoldenrod')\n",
    "        whiskerprops = dict(linestyle='-',linewidth=1.0, color='darkgoldenrod', alpha=0.5)\n",
    "\n",
    "        step_1_data = np.array([x[feature_ix] for x in step_1_all])\n",
    "        step_2_data = np.array([x[feature_ix] for x in step_2_all])\n",
    "        step_3_data = np.array([x[feature_ix] for x in step_3_all])\n",
    "        step_4_data = np.array([x[feature_ix] for x in step_4_all])\n",
    "        step_5_data = np.array([x[feature_ix] for x in step_5_all])\n",
    "        step_6_data = np.array([x[feature_ix] for x in step_6_all])\n",
    "        step_7_data = np.array([x[feature_ix] for x in step_7_all])\n",
    "        step_8_data = np.array([x[feature_ix] for x in step_8_all])\n",
    "        step_9_data = np.array([x[feature_ix] for x in step_9_all])\n",
    "        step_10_data = np.array([x[feature_ix] for x in step_10_all])\n",
    "        \n",
    "        # step_1_data[np.isnan(step_1_data)] = 1\n",
    "        # step_2_data[np.isnan(step_2_data)] = 1\n",
    "        # step_3_data[np.isnan(step_3_data)] = 1\n",
    "        # step_4_data[np.isnan(step_4_data)] = 1\n",
    "        # step_5_data[np.isnan(step_5_data)] = 1\n",
    "        # step_6_data[np.isnan(step_6_data)] = 1\n",
    "        # step_7_data[np.isnan(step_7_data)] = 1\n",
    "        # step_8_data[np.isnan(step_8_data)] = 1\n",
    "        # step_9_data[np.isnan(step_9_data)] = 1\n",
    "        # step_10_data[np.isnan(step_10_data)] = 1\n",
    "        \n",
    "        if feature_ix == 0:\n",
    "            d_jac = [step_1_data, step_2_data, step_3_data, step_4_data, step_5_data, step_6_data, step_7_data, step_8_data, step_9_data, step_10_data]\n",
    "            pos_jac = interp_factors_MSE[1:-1]\n",
    "            ax.boxplot(d_jac, positions=pos_jac, showfliers=False, widths=0.03, boxprops=boxprops, whiskerprops=whiskerprops)\n",
    "\n",
    "        else:\n",
    "            d_mse = [step_1_data, step_2_data, step_3_data, step_4_data, step_5_data, step_6_data, step_7_data, step_8_data, step_9_data, step_10_data]\n",
    "            pos_mse = interp_factors_MSE[1:-1]\n",
    "            ax.boxplot(d_mse, positions=pos_mse, showfliers=False, widths=0.03, boxprops=boxprops, whiskerprops=whiskerprops)\n",
    "            \n",
    "        \n",
    "        # violin_parts= ax.violinplot([step_1_data, step_2_data, step_3_data, step_4_data, step_5_data, step_6_data, step_7_data, step_8_data, step_9_data, step_10_data],\n",
    "        #         positions=interp_factors[1:-1], showmeans=False,\n",
    "        #           showmedians=True, widths=0.05)\n",
    "            \n",
    "        # draw a dashed line (45 degree line)\n",
    "        ax.plot([interp_factors[0], interp_factors[-1]], [0, 1], 'k--', alpha=0.5, label=\"Perfect Linear Interpolation\")\n",
    "        \n",
    "        ax.set_xlim(-0.2, 1.2)\n",
    "        ax.set_ylim(-0.2, 1.2)\n",
    "        ax.set_xticks(interp_factors, interp_labels, fontsize=10, rotation=0)\n",
    "        ax.set_yticks([*interp_factors, 1], np.round([*interp_factors, 1], 2), fontsize=10)\n",
    "        # remove grid\n",
    "        ax.grid(False)\n",
    "        # # draw vertical/horizontal lines for each step (in range(0, 1))\n",
    "        # for i in range(1, 11):\n",
    "        #     plt.scatter([interp_factors[i]], [interp_factors[i]], color=\"darkgoldenrod\", s=20)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # draw a line passing through center of the boxplots\n",
    "        ax.plot(interp_factors_MSE, np.median([np.zeros_like(step_1_data), step_1_data, step_2_data, step_3_data, step_4_data, step_5_data, step_6_data, step_7_data, step_8_data, step_9_data, step_10_data, np.ones_like(step_10_data)], axis=-1), \n",
    "                 color=\"darkgoldenrod\", label=\"Observed Interpolation\", alpha=0.5)\n",
    "        \n",
    "        # change label and title font size\n",
    "        # ax.set_xlabel(\"Interpolation Factor\", fontsize=12)\n",
    "        if feature_ix == 0:\n",
    "            ax.set_title(\"Normalized Jaccard Distance\", fontsize=12)\n",
    "        else:\n",
    "            ax.set_title(\"Normalized Velocity MSE\", fontsize=12)\n",
    "        \n",
    "    ax.legend(fontsize=10, loc=\"lower right\")\n",
    "    # rotate the xticks by 45 degrees\n",
    "        \n",
    "    return d_jac, pos_jac, d_mse, pos_mse\n",
    "plt.show()"
   ],
   "id": "d3553b3562d992af",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:35:13.281597Z",
     "start_time": "2024-04-11T07:29:39.247973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(9, 4.5), sharey=False)\n",
    "d_jac, pos_jac, d_mse, pos_mse = interpolation_plot(model_name=\"MuteGenre1\", beta=0.5, axs=axs)\n",
    "\n",
    "y = []\n",
    "x = []\n",
    "for ix, set_ in enumerate(d_jac):\n",
    "    for val in set_:\n",
    "        x.append(pos_jac[ix])\n",
    "        y.append(val)\n",
    "\n",
    "y = np.array(y)\n",
    "x = np.array(x)\n",
    "\n",
    "# replace nan values with 1\n",
    "# y[np.isnan(y)] = 1\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "res = stats.spearmanr(x, y)\n",
    "spearman_correlation_jac = res.statistic \n",
    "\n",
    "y = []\n",
    "x = []\n",
    "for ix, set_ in enumerate(d_mse):\n",
    "    for val in set_:\n",
    "        x.append(pos_mse[ix])\n",
    "        y.append(val)\n",
    "\n",
    "y = np.array(y)\n",
    "x = np.array(x)\n",
    "\n",
    "# replace nan values with 1\n",
    "# y[np.isnan(y)] = 1\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "res = stats.spearmanr(x, y)\n",
    "spearman_correlation_mse = res.statistic\n",
    "\n",
    "print(f\"Spearmans Correlation Jaccard: {np.round(spearman_correlation_jac, 2)}\")\n",
    "print(f\"Spearmans Correlation Jaccard: {np.round(spearman_correlation_mse,2)}\")\n",
    "# remove y axis tick labels\n",
    "axs[1].set_yticklabels([])\n",
    "#\n",
    "fig.savefig(\"./results/interp_plots_with_spears.png\", dpi=500)\n",
    "\n"
   ],
   "id": "17b8976637d9c1f2",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T07:38:15.827595Z",
     "start_time": "2024-04-11T07:38:15.415702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# make the gap between the axes smaller\n",
    "fig.subplots_adjust(wspace=0)\n",
    "fig.savefig(\"./results/interp_plots_with_spears.png\", dpi=500)"
   ],
   "id": "c43a316ced30d902",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "0",
   "id": "74e43896e2121c46",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "groovetransformer",
   "language": "python",
   "display_name": "GrooveTransformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
