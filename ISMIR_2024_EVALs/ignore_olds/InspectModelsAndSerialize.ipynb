{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc3d03bb57e4833a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T15:23:04.157996Z",
     "start_time": "2024-03-23T15:22:56.052212Z"
    }
   },
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%cd ..\n",
    "\n",
    "from helpers import load_model\n",
    "from model import BaseVAE, MuteVAE, MuteGenreLatentVAE, GenreClassifier\n",
    "from data.src.dataLoaders import Groove2Drum2BarDataset\n",
    "\n",
    "import torch\n",
    "\n",
    "from helpers.eval_utils import UMapper"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "down_sampled_ratio=None\n",
    "# load dataset as torch.utils.data.Dataset\n",
    "dataset = Groove2Drum2BarDataset(\n",
    "    dataset_setting_json_path=\"data/dataset_json_settings/Balanced_6000_performed.json\",\n",
    "    subset_tag=\"test\",\n",
    "    max_len=32,\n",
    "    tapped_voice_idx=2,\n",
    "    collapse_tapped_sequence=True,\n",
    "    num_voice_density_bins=3,\n",
    "    num_tempo_bins=6,\n",
    "    num_global_density_bins=7,\n",
    "    augment_dataset=False,\n",
    "    force_regenerate=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T15:23:05.440934Z",
     "start_time": "2024-03-23T15:23:04.158839Z"
    }
   },
   "id": "3e8ba580717efeb7",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from helpers import download_model_from_wandb, predict_using_model, load_model\n",
    "    \n",
    "# download_model_from_wandb(\"45\", 3, \"driven-frost-24\", GenreClassifier, new_path=\"./trained_models/genre_classifier.pth\")\n",
    "# download_model_from_wandb(\"155\", 1, \"lively-pond-9\", BaseVAE, new_path=\"./trained_models/base_vae_beta_0_2.pth\")\n",
    "# download_model_from_wandb(\"405\", 0, \"polished-pyramid-1\", MuteVAE, new_path=\"./trained_models/mute_vae_beta_0_2.pth\")\n",
    "\n",
    "genre_classifier = load_model(\"./trained_models/genre_classifier.pth\", GenreClassifier)\n",
    "model_BaseVAE = load_model(\"./trained_models/base_vae_beta_0_2.pth\", BaseVAE)\n",
    "model_MuteVAE = load_model(\"./trained_models/mute_vae_beta_0_2.pth\", MuteVAE)\n",
    "\n",
    "# model_MuteVAE\n",
    "\n",
    "\n",
    "# model.serialize(save_folder=f\"{run_name}\", filename=f\"Gen_{run_name}_{epoch}_serialized__{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T15:23:06.877365Z",
     "start_time": "2024-03-23T15:23:05.441727Z"
    }
   },
   "id": "c2efbf237b8655be",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_umap(model_, dataset, indices=None):\n",
    "    \"\"\"\n",
    "    Generate the umap for the given model and dataset setting.\n",
    "    Args:\n",
    "        dataset: torch.utils.data.Dataset\n",
    "        model_: torch.nn.Module\n",
    "        indices: list of int, optional\n",
    "            indices of the dataset to be used for umap generation\n",
    "\n",
    "    Returns:\n",
    "        dictionary ready to be logged by wandb {f\"{subset_name}_{umap}\": wandb.Html}\n",
    "    \"\"\"\n",
    "\n",
    "    _, latents_z = predict_using_model(model_, dataset, indices=indices)\n",
    "    tags = [dataset.genre_tags[i] for i in indices] if indices is not None else dataset.genre_tags\n",
    "    \n",
    "    umapper = UMapper(\"-\")\n",
    "    umapper.fit(latents_z.detach().cpu().numpy(), tags_=tags)\n",
    "    p = umapper.plot(show_plot=False, prepare_for_wandb=False)\n",
    "    return p\n",
    "\n",
    "def plot_and_synthesize(hvo_seq_sample):\n",
    "  \"\"\" Plots the piano roll of the sequence stored in the hvo_sequence object\n",
    "  and also returns the synthesized pattern\n",
    "  \"\"\"\n",
    "  hvo_seq_sample.piano_roll(show_figure=True)\n",
    "  audio = hvo_seq_sample.synthesize(\n",
    "      sf_path=\"hvo_sequence/soundfonts/Standard_Drum_Kit.sf2\")\n",
    "  return audio\n",
    "\n",
    "def get_sample_with_filename(name):\n",
    "    indices = [i for i, sample in enumerate(dataset.hvo_sequences) if name in sample.metadata[\"full_midi_filename\"]]\n",
    "    return indices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T15:23:06.882818Z",
     "start_time": "2024-03-23T15:23:06.878598Z"
    }
   },
   "id": "bccaa1f0bf37cde6",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run Inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db019fac53b58c42"
  },
  {
   "cell_type": "code",
   "source": [
    "from helpers import synthesize_visualize_using_models\n",
    "\n",
    "tabs, audios, gt_audio = synthesize_visualize_using_models([model_BaseVAE, model_MuteVAE], dataset, np.random.randint(0, len(dataset)))\n",
    "\n",
    "import IPython.display as ipd\n",
    "from bokeh.io import show\n",
    "\n",
    "audio_players = [ipd.Audio(gt_audio, rate=44100)]\n",
    "audio_players.extend([ipd.Audio(audio, rate=44100) for audio in audios])\n",
    "ipd.display(*audio_players)\n",
    "show(tabs)\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T15:23:07.467363Z",
     "start_time": "2024-03-23T15:23:06.883425Z"
    }
   },
   "id": "f711faa608fb7d9c",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\"# Generate Random Styles"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0b9ce50ddcd7b8a"
  },
  {
   "cell_type": "code",
   "source": [
    "latent_dim = 128\n",
    "z_a_dataset_ix = 0\n",
    "z_b_dataset_ix = 1\n",
    "print(\"Generated latent_z: \")\n",
    "print(\"-\"*50)\n",
    "print(\"re-run this cell to generate a new latent_z\")\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox\n",
    "import IPython.display as ipd\n",
    "from bokeh.embed import file_html\n",
    "from bokeh.resources import CDN\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T15:23:07.470752Z",
     "start_time": "2024-03-23T15:23:07.468146Z"
    }
   },
   "id": "ddbd8b6fc4987a85",
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "# Assuming 'dataset' and 'model' are defined elsewhere, along with other required imports\n",
    "from bokeh.io import show\n",
    "from bokeh.layouts import row\n",
    "global prev_randomize\n",
    "randomize_a_prev_state = False\n",
    "randomize_b_prev_state = False\n",
    "\n",
    "@interact(\n",
    "    genre=dataset.genre_tags,\n",
    "    kick_is_muted=widgets.Checkbox(value=False, description='Mute Kick'),\n",
    "    snare_is_muted=widgets.Checkbox(value=False, description='Mute Snare'),\n",
    "    hat_is_muted=widgets.Checkbox(value=False, description='Mute Hat'),\n",
    "    tom_is_muted=widgets.Checkbox(value=False, description='Mute Tom'),\n",
    "    cymbal_is_muted=widgets.Checkbox(value=False, description='Mute Cymbal'),\n",
    "    hsliderInterpolation=widgets.FloatSlider(value=0, min=0, max=1, step=0.01, description='Interpolation'),\n",
    "    randomizeA=widgets.Checkbox(value=False, description='Press to randomize'),\n",
    "    randomizeB=widgets.Checkbox(value=False, description='Press to randomize'))\n",
    "def generate(genre, kick_is_muted, snare_is_muted, hat_is_muted, tom_is_muted, cymbal_is_muted, hsliderInterpolation, randomizeA, randomizeB):\n",
    "    global prev_randomize, latent_z_A, latent_z_B, randomize_a_prev_state, randomize_b_prev_state\n",
    "    global genre_classifier, model_BaseVAE, model_MuteVAE, dataset, z_a_dataset_ix, z_b_dataset_ix\n",
    "    \n",
    "    if randomizeA != randomize_a_prev_state:\n",
    "            # grab a random sample from the dataset\n",
    "        z_a_dataset_ix = np.random.randint(0, len(dataset))\n",
    "        randomize_a_prev_state = randomizeA\n",
    "    elif randomizeB != randomize_b_prev_state:\n",
    "        z_b_dataset_ix = np.random.randint(0, len(dataset))\n",
    "        randomize_b_prev_state = randomizeB\n",
    "        \n",
    "    audios = []\n",
    "    plots = []\n",
    "    genres_classified = []\n",
    "    \n",
    "    # Convert binary mute selections to tensors\n",
    "    mutes = torch.tensor([\n",
    "        kick_is_muted,\n",
    "        snare_is_muted,\n",
    "        hat_is_muted,\n",
    "        tom_is_muted,\n",
    "        cymbal_is_muted\n",
    "    ]).long()\n",
    "    \n",
    "    \n",
    "    for m_ in [model_BaseVAE, model_MuteVAE]:\n",
    "        \n",
    "        _, latent_z_A = predict_using_model(m_, dataset, indices=[z_a_dataset_ix])\n",
    "        _, latent_z_B = predict_using_model(m_, dataset, indices=[z_b_dataset_ix])\n",
    "        \n",
    "        # Interpolate between latent_z_A and latent_z_B\n",
    "        latent_z = latent_z_A * (1 - hsliderInterpolation) + latent_z_B * hsliderInterpolation\n",
    "            \n",
    "        genre_ix = dataset.genre_tags.index(genre)\n",
    "        genre_ix = torch.tensor([genre_ix], dtype=torch.long)\n",
    "        \n",
    "        if m_ is model_BaseVAE:\n",
    "            \n",
    "            # Sample from the model using the updated latent_z\n",
    "            # Note: Implement the model's sampling logic here\n",
    "            # decode\n",
    "            h, v, o = m_.sample(\n",
    "                latent_z = latent_z,\n",
    "                voice_thresholds=torch.tensor([0.5] * 9),\n",
    "                voice_max_count_allowed=torch.tensor([32] * 9),\n",
    "                sampling_mode=0\n",
    "            )\n",
    "        elif m_ is model_MuteVAE:\n",
    "            h, v, o = m_.sample(\n",
    "                latent_z = latent_z,\n",
    "                kick_is_muted=torch.tensor([mutes[0]]),\n",
    "                snare_is_muted=torch.tensor([mutes[1]]),\n",
    "                hat_is_muted=torch.tensor([mutes[2]]),\n",
    "                tom_is_muted=torch.tensor([mutes[3]]),\n",
    "                cymbal_is_muted=torch.tensor([mutes[4]]),\n",
    "                voice_thresholds=torch.tensor([0.5] * 9),\n",
    "                voice_max_count_allowed=torch.tensor([32] * 9),\n",
    "                sampling_mode=0\n",
    "            )\n",
    "            \n",
    "        hvo = torch.cat([h, v, o], dim=2)\n",
    "        \n",
    "        gen_, _ = genre_classifier.predict(hvo)\n",
    "        genres_classified.append(gen_)\n",
    "        \n",
    "        hvo_seq = dataset.hvo_sequences[0].copy_empty()\n",
    "        hvo_seq.hvo = hvo[0, :, :].squeeze().detach().cpu().numpy()\n",
    "        \n",
    "        pr = hvo_seq.piano_roll(width=700, height=300, filename=f\"{m_.__class__.__name__}\")\n",
    "        # convert bokeh figure to html\n",
    "        \n",
    "        plots.append(pr)\n",
    "        \n",
    "        audio = hvo_seq.synthesize(sf_path=\"hvo_sequence/soundfonts/Standard_Drum_Kit.sf2\")\n",
    "        audios.append(audio)\n",
    "    \n",
    "\n",
    "    print(\"Pattern A (gt) genre: \", dataset.genre_targets[z_a_dataset_ix])\n",
    "    print(\"Pattern B (gt) genre: \", dataset.genre_targets[z_b_dataset_ix])  \n",
    "    print(f\"Genre classified by BaseVAE: {genres_classified[0]}\")\n",
    "    print(f\"Genre classified by MuteVAE: {genres_classified[1]}\")\n",
    "    # show html\n",
    "    html = file_html(row(*plots), CDN, \"my plot\")\n",
    "    for audio in audios:\n",
    "        ipd.display(ipd.Audio(audio, rate=44100))\n",
    "    \n",
    "    ipd.display(ipd.HTML(html))\n",
    "    \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T15:23:07.888677Z",
     "start_time": "2024-03-23T15:23:07.471480Z"
    }
   },
   "id": "71d017c21dac3ef2",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T15:23:07.890575Z",
     "start_time": "2024-03-23T15:23:07.889307Z"
    }
   },
   "id": "ae5b20a0572f51e1",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# import torch\n",
    "# import ipywidgets as widgets\n",
    "# from ipywidgets import interact_manual, VBox, HBox\n",
    "# import numpy as np\n",
    "# import IPython.display as ipd\n",
    "# from IPython.display import display\n",
    "# \n",
    "# # Assuming 'model', 'dataset', and other necessary components are defined elsewhere\n",
    "# \n",
    "# latent_dim = 128  # Set this to match your model's latent dimension\n",
    "# latent_z = torch.randn(1, latent_dim)  # Initialize latent_z\n",
    "# \n",
    "# # Create sliders\n",
    "# sliders = [widgets.FloatSlider(min=-3, max=3, value=0, step=0.01, orientation='vertical', readout=False, layout={'width': '20px', 'height': '75px'}) for _ in range(latent_dim)]\n",
    "# \n",
    "# # initialize sliders with latent_z values\n",
    "# for i, slider in enumerate(sliders):\n",
    "#     slider.value = latent_z[0, i].item()\n",
    "#     \n",
    "# # Function to update latent_z based on sliders' values\n",
    "# def update_latent_z(change):\n",
    "#     global latent_z\n",
    "#     latent_z[:] = torch.tensor([[slider.value for slider in sliders]], dtype=torch.float)\n",
    "# \n",
    "# # Attach update function to sliders\n",
    "# for slider in sliders:\n",
    "#     slider.observe(update_latent_z, names='value')\n",
    "# \n",
    "# # Group sliders into horizontal boxes\n",
    "# num_sliders_per_row = 64  # Adjust this number based on your display preferences\n",
    "# hboxes = [HBox(sliders[i:i+num_sliders_per_row]) for i in range(0, latent_dim, num_sliders_per_row)]\n",
    "# \n",
    "# # Display the grouped sliders\n",
    "# vbox = VBox(hboxes)\n",
    "# display(vbox)\n",
    "# \n",
    "# # Define the generate function to use the updated latent_z\n",
    "# def generate(genre, global_density, kick_is_muted, snare_is_muted, hat_is_muted, tom_is_muted, cymbal_is_muted):\n",
    "#     global latent_z\n",
    "#     \n",
    "#     genre_ix = dataset.genre_tags.index(genre)\n",
    "#     genre_ix = torch.tensor([genre_ix], dtype=torch.long)\n",
    "#     \n",
    "#     # Convert binary mute selections to tensors\n",
    "#     mutes = torch.tensor([\n",
    "#         kick_is_muted,\n",
    "#         snare_is_muted,\n",
    "#         hat_is_muted,\n",
    "#         tom_is_muted,\n",
    "#         cymbal_is_muted\n",
    "#     ]).long()\n",
    "#     \n",
    "#     # Sample from the model using the updated latent_z\n",
    "#     # Note: Implement the model's sampling logic here\n",
    "#     # decode\n",
    "#     h, v, o = model.sample(\n",
    "#         latent_z = latent_z,\n",
    "#         genre= genre_ix,\n",
    "#         kick_is_muted=torch.tensor([mutes[0]]),\n",
    "#         snare_is_muted=torch.tensor([mutes[1]]),\n",
    "#         hat_is_muted=torch.tensor([mutes[2]]),\n",
    "#         tom_is_muted=torch.tensor([mutes[3]]),\n",
    "#         cymbal_is_muted=torch.tensor([mutes[4]]),\n",
    "#         voice_thresholds=torch.tensor([0.5] * 9),\n",
    "#         voice_max_count_allowed=torch.tensor([32] * 9),\n",
    "#         sampling_mode=0\n",
    "#     )\n",
    "#     \n",
    "#     hvo = torch.cat([h, v, o], dim=2)\n",
    "#     \n",
    "#     hvo_seq = dataset.hvo_sequences[0].copy_empty()\n",
    "#     hvo_seq.hvo = hvo[0, :, :].squeeze().detach().cpu().numpy()\n",
    "#     \n",
    "#     pr = hvo_seq.piano_roll(width=700, height=300)\n",
    "#     # convert bokeh figure to html\n",
    "#     \n",
    "#     html = file_html(pr, CDN, \"my plot\")\n",
    "#     \n",
    "#     audio = hvo_seq.synthesize(sf_path=\"hvo_sequence/soundfonts/Standard_Drum_Kit.sf2\")\n",
    "#     IPython.display.display(ipd.Audio(audio, rate=44100))\n",
    "#     \n",
    "#     # show html\n",
    "#     IPython.display.display(IPython.display.HTML(html))\n",
    "#     \n",
    "# # Setup interactive widgets for the generate function\n",
    "# interact_manual(generate,\n",
    "#     genre=dataset.genre_tags,\n",
    "#     global_density=(0, 9),\n",
    "#     kick_is_muted=widgets.Checkbox(value=False),\n",
    "#     snare_is_muted=widgets.Checkbox(value=False),\n",
    "#     hat_is_muted=widgets.Checkbox(value=False),\n",
    "#     tom_is_muted=widgets.Checkbox(value=False),\n",
    "#     cymbal_is_muted=widgets.Checkbox(value=False)\n",
    "# )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T15:23:07.894006Z",
     "start_time": "2024-03-23T15:23:07.891173Z"
    }
   },
   "id": "cfd98583c18058e0",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T15:23:07.896238Z",
     "start_time": "2024-03-23T15:23:07.894999Z"
    }
   },
   "id": "9601467d950ba6e5",
   "execution_count": 8,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "groovetransformer",
   "language": "python",
   "display_name": "GrooveTransformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
